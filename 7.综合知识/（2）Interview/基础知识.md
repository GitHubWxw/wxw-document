## **基础知识** 

### 操作系统

1. 进程和线程的区别是什么？进程之间和线程之间如何通信？

### 缓存

#### 1 Redis

1. redis 集群各节点之间如何通信（what/why/how）


### 中间件

#### 1. 消息队列

##### 1.1 消息队列概述

- 你用过消息队列么？
- 说说你们项目里是怎么用消息队列的？
  - 我们有一个订单系统，订单系统会每次下一个新订单的时候，就会发送一条消息到ActiveMQ里面去，后台有一个库存系统，负责获取消息，然后更新库存。
- 为什么使用消息队列？
  - 你的订单系统不发送消息到MQ，而是直接调用库存系统的一个接口，然后直接调用成功了，库存也更新了，那就不需要使用消息队列了呀
  - 使用消息队列的主要作用是：异步、解耦、削峰
- 消息队列都有什么优缺点？
- Kafka、activeMQ、RibbitMQ、RocketMQ都有什么优缺点？
- 如何保证消息队列的高可用？
- 如何保证消息不被重复消费？如何保证消息消费时的幂等性？
- 如何保证消息的可靠性传输，要是消息丢失了怎么办？
- 如何保证消息的顺序性？
- 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？
- 如果让你写一个消息队列，该如何进行架构设计，说一下你的思路？

面试官问的问题不是发散的，而是从点、铺开，比如先聊一聊高并发的话题，就这个话题里面继续聊聊缓存、MQ等等东西。对于每个小话题，比如说MQ，就会从浅入深。

##### 1.2 为什么使用消息队列？

优点

- 消息队列的场景使用场景很多，主要是三个：解耦、异步、和削峰

缺点

- 系统可用性降低：系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统接口就好了，人家ABCD四个系统好好的，没啥问题，这个时候却加入了MQ进来，万一MQ挂了怎么办？MQ挂了整套系统也会崩溃了。
- 系统复杂性提高：硬生生加个MQ进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？
- 一致性问题：A系统处理完了直接返回成功了，人都以为你的请求成功了，但是问题是，要在BCD三个系统中，BD两个系统写库成功了，结果C系统写库失败了，这样就会存在数据不一致的问题。
- 所以说消息队列实际上是一种复杂的架构，你引入它有好多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，最后发现系统复杂性提升了一个数量级，也许是复杂10倍，但是关键时刻，用还是得用。

> 主流MQ包括：kafka、ActiveMQ、RabbitMQ和RocketMQ

| 特性       | ActiveMQ                                                     | RabbitMQ                                                     | RocketMQ                                                     | Kafka                                                        |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量 | 万级，吞吐量比RocketMQ和Kafka要低一个数量级                  | 万级，吞吐量比RocketMQ和Kafka要低一个数量级                  | 10万级，RocketMQ也是可以支撑高吞吐的一种MQ                   | 10万级1这是kafka最大的优点，就是吞吐量高。一般配置和数据类的系统进行实时数据计算、日志采集等场景 |
| 时效性     | ms级                                                         | 微妙级，这是RabbitMQ的一大特点，就是延迟最低                 | ms级                                                         | 延迟在ms级内                                                 |
| 可用性     | 基于主从架构实现高可用                                       | 高，基于主从架构实现高可用                                   | 非常高，分布式架构                                           | 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机后，不会丢失数据，不会导致不可用 |
| 消息可靠性 | 有较低的概率丢失数据                                         | 消息不丢失                                                   | 经过参数优化配置，可以做到0丢失                              | 经过参数优化配置可以做到0丢失                                |
| 核心特点   | MQ领域的功能及其完备                                         | 基于Erlang开发，所以并发能力强，性能及其好，延时很低         | MQ功能较为完善，还是分布式的，扩展性好                       | 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是实时上的标准。 |
|            | 非常成熟，功能强大，在业内大量公司以及项目都有应用。 但是偶尔消息丢失的概率，并且现在社区以及国内应用都越来越少，官方社区对ActiveMQ5.X维护越来越少，而且确实主要是基于解耦和异步来用的，较少在大规模吞吐场景中使用 | erlang语言开发的，性能及其好，延时很低。而且开源的版本，就提供的管理界面非常棒，在国内一些互联网公司近几年用RabbitMQ也是比较多一些，特别适用于中小型的公司 缺点显而易见，就是吞吐量会低一些，这是因为它做的实现机制比较中，因为使用erlang开发，目前没有多少公司使用其开发。所以针对源码界别的定制，非常困难，因此公司的掌控非常弱，只能依赖于开源社区的维护。 | 接口简单易用，毕竟在阿里大规模应用过，有阿里平台保障，日处理消息上 百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是OK的，还可以支撑大规模的topic数量，支持复杂MQ业务场景。 | 仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级别的延迟，极高的可用性以及可靠性，分布式可以任意扩展。 同时kafka最好是支撑较少的topic数量即可，保证其超高的吞吐量。 |

综上所述：

- 一般的业务要引入MQ，最早大家都是用ACviceMQ，但是现在大家用的不多了，没有经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了，不太图鉴使用
- RabbitMQ后面被大量的中小型公司所使用，但是erlang语言阻碍了大量的Java工程师深入研究和掌握它，对公司而言，几乎处于不可控的状态，但是RabbitMQ目前开源稳定，活跃度也表较高。
- RocketMQ是阿里开源的一套消息中间件，目前也已经经历了天猫双十一，同时底层使用Java进行开发

如果中小型企业技术实力一般，技术挑战不是很高，可以推荐，RabbitMQ。如果公司的基础研发能力很强，想精确到源码级别的掌握，那么推荐使用RocketMQ。同时如果项目是聚焦于大数据领域的实时计算，日志采集等场景，那么Kafka是业内标准。

##### 2.2 如何保证消息队列的高可用？

- RabbitMQ 高可用：

  三种模式：单机模式，普通集群模式，**镜像集群模式**【实现高可用】

- kafka实现高可用

  kafka一个最基本的架构认识：

  ​       多个broker组件，每个broker是一个节点，你创建一个topic，这个topic可以划分成多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。这就是天然的分布式消息队列，就是说一个topic的数据，是分散在多个机器上的，每个机器上就放一部分数据。

  **高可用**：提供了HA机制（HA双机集群），就是replica副本机制，每个partition的数据都会同步到其它机器上，形成自己的多个replica副本，然后所有的replica就是follower，写的时候，leader会负责数据都同步到所有的follower上，读的时候就直接读取leader上的数据即可。只能读写leader？很简单，要是你能随意读写每个follower，那么就需要保证数据一致性的问题，系统复杂度太高，很容易出问题，kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才能够提高容错性。

  ​     每个副本不会存储节点的全部数据，而是数据可能分布在不同的机器上。同时多个副本中，会选取一个作为leader，其它的副本是作为follower，并且只有leader能对外提供读写，同时leader在写入数据后，它还会把全部的数据同步到follower中，保证数据的备份。

  此时，高可用的架构就出来了，假设现在某个机器宕机了，比如其中的一个leader宕机了，但是因为每个leader下还有多个follower，并且每个follower都进行了数据的备份，因此kafka会自动感知leader已经宕机，同时将其它的follower给选举出来，作为新的leader，并向外提供服务支持。

##### 2.3 [如果保证消息的重复消费？](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%e5%a6%82%e6%9e%9c%e4%bf%9d%e8%af%81%e6%b6%88%e6%81%af%e7%9a%84%e9%87%8d%e5%a4%8d%e6%b6%88%e8%b4%b9%ef%bc%9f) 

> 面试题：如何保证消息的重复消费？如何保证消息消费的幂等性？

- Kafka 如何保证幂等性

kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我从上次消费到的offset来继续消费。但是凡事总有以外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启，如果碰到着急的，直接kill杀死进程，然后重启，这就会**导致consumer有些消息处理了没来得及提交offset**，然后重启后，就会造成少数消息重复消费的问题。

![image-20200420112217458](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/image-20200420112217458.png)

消费者如果在准备提交offset，但是还没有提交的时候，消费者进程被重启，那么此时已经消费过数据的offset并没有提交，kafka也就不知道你已经消费了，那么消费者再次上线进行消费的时候，会把已经消费的数据，重新在传递过来，这就是消息重复消费的问题。

解决思路：

- 比如那个数据要写库，首先根据主键查一下，如果这个数据已经有了，那就别插入了，执行update即可
- 如果用的是redis，那就没问题了，因为每次都是set操作，天然的幂等性
- 如果不是上面的两个场景，那就做的稍微复杂一点，需要让生产者发送每条消息的时候，需要加一个全局唯一的id，类似于订单id之后的东西，然后你这里消费到了之后，先根据这个id去redis中查找，之前消费过了么，如果没有消费过，那就进行处理，然后把这个id写入到redis中，如果消费过了，那就别处理了，保证别重复消费相同的消息即可。
- 还有比如基于数据库唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会重复，因为Kafka消费者还没来得及提交offset，重复数据拿到了以后，我们进行插入的时候，因为有了**唯一键约束**了，所以重复数据只会插入报错，不会导致数据库中出现脏数据。

##### 2.4 [如何保证消息传输不丢失？](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%e5%a6%82%e4%bd%95%e4%bf%9d%e8%af%81%e6%b6%88%e6%81%af%e4%bc%a0%e8%be%93%e4%b8%8d%e4%b8%a2%e5%a4%b1%ef%bc%9f) 

> 面试题：如何保证消息的可靠性传输（如何处理消息丢失的问题）？

消息队列有三个重要原则：消息不能多，不能少

不能多，指的就是刚刚提到的**重复消费和幂等性问题**，不能少，指的是数据在传输过程中，**不会丢失**。

消息丢失类型：1. MQ自己丢了。2. 消费的时候丢了

**解决思路：**

1. 一般是用confirm机制，因为是异步的模式，在发送消息之后，不会阻塞，直接可以发送下一条消息，这样吞吐量会更高一些。【生产者给消息队列发消息过程丢失消息】
2. 开启RabbitMQ的持久化，就是消息写入之后，同时需要持久化到磁盘中，哪怕是RabbitMQ自己宕机了，也能够从磁盘中读取之前存储的消息，这样数据一般就不会丢失了。【MQ队列中丢失消息】
   - 但是存在一个极端的情况，就是RabbitMQ还没持久化的时候，就已经宕机了，那么可能会造成少量的数据丢失，但是这个概率是比较小的。
   - 持久化可以跟生产者那边的confirm机制配置起来，只有消息被持久化到磁盘后，才会通知生产者ACK了，所以哪怕是在持久化磁盘之前，RabbitMQ挂了，数据丢了，生产者收不到ACK，你也是可以自己重发的。
3. 需要将AutoAck给关闭（自动确认关闭），然后每次自己确定已经处理完了一条消息后，你再发送ack给RabbitMQ，如果你还没处理完就宕机了，此时RabbitMQ没收到你发的Ack消息，然后RabbitMQ就会将这条消息分配给其它的消费者去处理。【消费者丢失消息】

##### 2.5 如何保证消息的顺序性？

- RabbitMQ：一个queue，多个consumer，这不明显乱了
- Kafka：一个topic，一个partition，一个consumer，内部多线程，就会乱套

1. RabbitMQ保证消息的顺序性

   拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦，或者就是一个queue，但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理。

2. Kafka 保证消息的顺序性

   一个topic，一个partition，一个consumer，内部单线程消费，写N个内存，然后N个线程分别消费一个内存queu即可。注意，kafka中，写入一个partition中的数据，一定是有顺序的，但是在一个消费者的内部，假设有多个线程并发的进行数据的消费，那么这个消息又会乱掉。

   这个时候，我们需要引入内存队列，然后我们通过消息的key，然后我们通过hash算法，进行hash分发，将相同订单key的散列到我们的同一个内存队列中，然后每一个线程从这个Queue中拉数据，同一个内存Queue也是有顺序的。

##### 2.6 [百万消息挤压如何处理？](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%E7%99%BE%E4%B8%87%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E5%9C%A8%E9%98%9F%E5%88%97%E4%B8%AD%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%EF%BC%9F) 

> 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有百万消息积压接小时，说说解决思路？

**场景1：积压大量消息** 

几千万的消息积压在MQ中七八个小时，这也是一个真实遇到过的一个场景，确实是线上故障了，

只能够做紧急的扩容操作了，具体操作步骤和思路如下所示：

- 先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停止
- 临时建立好原先10倍或者20倍的queue数量
- 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue
- 接着临时征用10倍机器来部署consumer，每一批consumer消费一个临时queue的数据
- 这种做法相当于临时将queue资源和consumer资源扩大了10倍，以正常的10倍速度

> 也就是让消费者把消息，重新写入MQ中，然后在用 10倍的消费者来进行消费。

**[场景2：大量消息积压，并且设置了过期时间](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%e5%9c%ba%e6%99%af2%ef%bc%9a%e5%a4%a7%e9%87%8f%e6%b6%88%e6%81%af%e7%a7%af%e5%8e%8b%ef%bc%8c%e5%b9%b6%e4%b8%94%e8%ae%be%e7%bd%ae%e4%ba%86%e8%bf%87%e6%9c%9f%e6%97%b6%e9%97%b4)**  

> 假设你用的是RabbitMQ，RabbitMQ是可以设置过期时间的，就是TTL，如果消息在queue中积压超过一定的时间，就会被RabbitMQ给清理掉，这个数据就没了。这个时候就不是数据被大量积压的问题，而是大量的数据被直接搞丢了。

这种情况下，就不是说要增加consumer消费积压的消息，因为实际上没有啥积压的，而是丢了大量的消息，我们可以采取的一个方案就是，批量重导，这个之前线上也有遇到类似的场景，就是大量的消息积压的时候，然后就直接丢弃了数据，然后等高峰期过了之后，例如在晚上12点以后，就开始写程序，将丢失的那批数据，写个临时程序，一点点查询出来，然后重新 添加MQ里面，把白天丢的数据，全部补回来。

假设1万个订单积压在MQ里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单查询出来，然后手动发到MQ里面去再补一次。

**[场景3：大量消息积压，导致MQ磁盘满了](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/1_%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%e5%9c%ba%e6%99%af3%ef%bc%9a%e5%a4%a7%e9%87%8f%e6%b6%88%e6%81%af%e7%a7%af%e5%8e%8b%ef%bc%8c%e5%af%bc%e8%87%b4mq%e7%a3%81%e7%9b%98%e6%bb%a1%e4%ba%86)**   

> 如果是消息积压在MQ里，如果你很长时间都没有处理掉，此时导致MQ都快写满了，咋办？

也是因为场景一执行的太慢了，只能写一个临时程序，接入数据来消费，然后消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到凌晨的时候，在把消息填入MQ中进行消费。

##### 2.7 如何设计一个消息中间件架构？

> 如果让你写一个消息队列，该如何进行架构设计？说下你的思路

**思路：**

- 首先**MQ得支持可伸缩性，那就需要快速扩容，就可以增加吞吐量和容量**，可以设计一个分布式的系统，参考kafka的设计理念，broker - > topic -> partition，每个partition放一台机器，那就存一部分数据，如果现在资源不够了，可以给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多的数据，提高更高的吞吐量。
- 其次得考虑一下这个MQ的数据**要不要落地磁盘**？也就是需不需要保证消息持久化，因为这样可以保证数据的不丢失，那落盘的时候怎么落？顺序写，这样没有磁盘随机读写的寻址开销，磁盘顺序读的性能是很高的，这就是kafka的思路。
- 其次需要**考虑MQ的可用性**？这个可以具体到我们上面提到的消息队列保证高可用，提出了多副本 ，leader 和follower模式，当一个leader宕机的时候，马上选取一个follower作为新的leader对外提供服务。
- 需不需要**支持数据0丢失**？可以参考kafka零丢失方案

其实一个MQ肯定是很复杂的，问这个问题其实是一个开放性问题，主要是想看看有没有从架构的角度整体构思和设计的思维以及能力

#### 2. 分布式搜索引擎

业内目前来说事实上的一个标准，就是分布式搜索引擎一般大家都是用ElasticSearch，（原来的话使用的是Solr），但是确实，这两年大家一般都用更加易用的es。

ElasticSearch 和 Solr 底层都是基于Lucene，而Lucene的底层原理是 **倒排索引**

##### 2.1 什么是倒排索引？

- 又叫反向索引，倒序索引，倒排索引适用于快速的全文检索，一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表

- 正常索引使用

  ```txt
  比如一张数据库表table，有id和content两个字段，表中数据如下：
  		id        content
  		1          abc
  		2           fg
  		3          ahu
  		4          bgf
  使用id=2进行索引查找fg。
  select from table where id=2;
  即可查询到相应的值。
  ```

- 问题

  ```txt
  现在我们可以根据数据库表中的id可以查询对应的值，但是有一些问题，
  当我们想要查询fg值的id是多少或者我们想要知道哪些conten中含有a（或者b）。

  这就是我们日常生活中经常使用的一个算法：当你搜索一首歌时，你可以不知道它的全名，
  但是当你输入其中一部分文字时，音乐平台会自动提示或者推荐了一些相似的歌名。
  ```

- 解决问题的算法-----倒排索引算法

  ```txt
  思路：（通俗的理解）
  		上述中的表table：
  			   id        content
  				1          abc
  				2           fg
  				3          ahu
  				4          bgf
  		我们的目的：通过content值或者content值中的某个字母查找对应的id
  		我们是不是可以根据上述的表table模拟出为下面的表table1：
  				content     id
  					a       1,3
  					b       1,4
  					c       1
  					f       2,4
  					g       2,4
  					h       3
  		模拟分析：新建表是按照字母作为索引进行新建，将之前table中的content值按照content值中含有的字母进行分类，比如
  		第一个content索引值为a，之前的table表中含有a的content
  有id=1（abc）,id=3(ahu)。

  这就是倒排索引算法的思路。
  ```

- 倒排索引使用的场景

  1. 全局搜索：比如在整个数据库中查找单词word
  2. 模糊查询
  3. 关键字查询

- 深入理解

  1. 假设文档集合中包含五个文档，每个文档的内容如下所示，在图中最左端一栏是每个文档对应的编号，我们的任务就是对这个文档集合建立倒排索引

  ![image-20200420174829608](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/image-20200420174829608.png)

  2. 中文和英文等语言不通，单词之间没有明确分割符号，所以首先要用**分词系统**将文档自动切分成单词序列，这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时**记录下哪些文档包含这个单词**，在如此处理结束后，我们就可以得到最简单的倒排索引了

     ![image-20200420175115061](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/image-20200420175115061.png) 

3. 索引系统还可以记录除此之外的更多信息，下图是记录了单词出现的频率（TF）即这个单词在文档中出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以便后续排序时进行分值计算。

   ![image-20200420175534595](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/image-20200420175534595.png) 

4. 倒排列表还可以记录单词在某个文档出现的位置信息

   ```java
   (1, <11>, 1), (2, <7>, 1), (3, <3, 9>, 2)
   ```

有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词 "Facebook"，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息，文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。

##### 2.2 [中文分词器原理](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d%e5%99%a8%e5%8e%9f%e7%90%86) 

**方法一：** 分词器的原理本质上是词典分词。在现有内存中初始化一个词典，然后在分词过程中挨个读取字符和字典中的字符相匹配，把文档中所有词语拆分出来的过程。

**方法二：** 字典树

Trie树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。

下面一个存放了[大学、大学生、学习、学习机、学生、生气、生活、活着]这个词典的trie树： ![image-20200630105344435](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/image-20200630105344435.png) 

> 参考地址：[中文分析基本原理](https://blog.csdn.net/wbsrainbow/article/details/88795312)

##### 2.3 ES分布式架构原理能说一下么

![01_elasticsearchåå¸å¼æ¶æåç](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/01_elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png) 

elasticsearch设计的理念就是分布式搜索引擎，底层其实还是基于lucene的。

- **核心思想**就是在多台机器上启动多个es进程实例，组成了一个es集群。

- ES中存储数据的基本单位是**索引**。

  **比如**说你现在要在es中存储一些订单数据，你就应该在es中创建一个索引，order_idx，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是mysql里的一张表。

  index -> type -> mapping -> document -> field。

  | 概念                 | 说明                                                         |
  | -------------------- | ------------------------------------------------------------ |
  | 索引库（indices)     | indices是index的复数，代表许多的索引，                       |
  | 类型（type）         | 类型是模拟mysql中的table概念，一个索引库下可以有不同类型的索引，比如商品索引，订单索引，其数据格式不同。不过这会导致索引库混乱，因此未来版本中会移除这个概念 |
  | 文档（document）     | 存入索引库原始的数据。比如每一条商品信息，就是一个文档       |
  | 字段（field）        | 文档中的属性                                                 |
  | 映射配置（mappings） | 字段的数据类型、属性、是否索引、是否存储等特性               |

- 索引集（Indices，index的复数）：逻辑上的完整索引 collection1
- 分片（shard）：数据拆分后的各个部分


- 副本（replica）：每个分片的复制

  ```json
  {
      "settings": {
          "number_of_shards": 3, // 分片数量
          "number_of_replicas": 2 // 副本数量
        }
  }
  ```

接着你搞一个索引，这个索引可以拆分成多个shard，每个shard存储部分数据。

- 接着就是这个shard的数据实际是有多个备份，就是说每个shard都有一个**primary shard**，主分片负责写入数据，但是还有几个replica shard。primary shard写入数据之后，会将数据同步到其他几个replica shard上去。
- 通过这个replica的方案，每个shard的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。
- es集群多个节点，会自动选举一个节点为master节点，这个master节点其实就是干一些管理的工作的，比如维护索引元数据拉，负责切换primary shard和replica shard身份拉，之类的。
- 如果是非master节点宕机了，那么会由master节点，让那个宕机节点上的primary shard的身份转移到其他机器上的replica shard。急着你要是修复了那个宕机机器，重启了之后，master节点会控制将缺失的replica shard分配过去，同步后续修改的数据之类的，让集群恢复正常。

其实上述就是elasticsearch作为一个分布式搜索引擎最基本的一个架构设计 [参考地址](https://blog.csdn.net/qq_41893274/article/details/105434222) 

##### 2.4 [ES查询和读取数据的工作原理是什么？](http://moxi159753.gitee.io/learningnotes/#/./%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/README?id=es%e6%9f%a5%e8%af%a2%e5%92%8c%e8%af%bb%e5%8f%96%e6%95%b0%e6%8d%ae%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88%ef%bc%9f)  

![01_esè¯»ååºå±åçåæ](http://moxi159753.gitee.io/learningnotes/%E6%A0%A1%E6%8B%9B%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E6%89%AB%E7%9B%B2%E5%AD%A6%E4%B9%A0/2_%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%9D%A2%E8%AF%95%E8%BF%9E%E7%8E%AF%E7%82%AE/images/01_es%E8%AF%BB%E5%86%99%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90.png) 

**（1）ES写数据的过程**

1. 客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点）
2. coordinating node，对document进行路由，将请求转发给对应的node（有primary shard）
3. 实际的node上的primary shard处理请求，然后将数据同步到replica node
4. coordinating node，如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端

**（2）ES读数据的过程**

查询，GET某一条数据，写入了某个document，这个document会自动给你分配一个全局唯一的id，doc id，同时也是根据doc id进行hash路由到对应的primary shard上面去。也可以手动指定doc id，比如用订单id，用户id。

你可以通过doc id来查询，会根据doc id进行hash，判断出来当时把doc id分配到了哪个shard上面去，从那个shard去查询

1. 客户端发送请求到任意一个node，成为coordinate node
2. coordinate node对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡
3. 接收请求的node返回document给coordinate node
4. coordinate node返回document给客户端

**（3）ES搜索数据的过程** 

ES最强大的是做全文检索，就是比如你有三条数据

```txt
java真好玩儿啊
java好难学啊
j2ee特别牛
你根据java关键词来搜索，将包含java的document给搜索出来
```

es就会给你返回：java真好玩儿啊，java好难学啊

1. 客户端发送请求到一个coordinate node
2. 协调节点将搜索请求转发到所有的shard对应的primary shard或replica shard也可以
3. query phase：每个shard将自己的搜索结果（其实就是一些doc id），返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果
4. fetch phase：接着由协调节点，根据doc id去各个节点上拉取实际的document数据，最终返回给客户端

**（4）搜索的底层原理，倒排索引，画图说明传统数据库和倒排索引的区别**

（5）写数据底层原理

##### 2.5 ES在数十亿级数据如何提高查询性能

说实话，es性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。

一块一块来分析吧

在这个海量数据的场景下，如何提升es搜索的性能，也是我们之前生产环境实践经验所得

**（1）性能优化的杀手锏——filesystem cache**

- os cache，操作系统的缓存

你往es里写的数据，实际上都写到磁盘文件里去了，磁盘文件里的数据操作系统会自动将里面的数据缓存到os cache里面去

es的搜索引擎严重依赖于底层的filesystem cache，你如果给filesystem cache更多的内存，尽量让内存可以容纳所有的indx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。

性能差距可以有大，我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒，5秒，10秒。但是如果是走filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。

之前有个学员，一直在问我，说他的搜索性能，聚合性能，倒排索引，正排索引，磁盘文件，十几秒。。。。

> 真实案例分析

比如说，你，es节点有3台机器，每台机器，看起来内存很多，64G，总内存，64 * 3 = 192g

每台机器给es jvm heap是32G，那么剩下来留给filesystem cache的就是每台机器才32g，总共集群里给filesystem cache的就是32 * 3 = 96g内存

我就问他，ok，那么就是你往es集群里写入的数据有多少数据量？

如果你此时，你整个，磁盘上索引数据文件，在3台机器上，一共占用了1T的磁盘容量，你的es数据量是1t，每台机器的数据量是300g

你觉得你的性能能好吗？filesystem cache的内存才100g，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差

当时他们的情况就是这样子，es在测试，弄了3台机器，自己觉得还不错，64G内存的物理机。自以为可以容纳1T的数据量。

归根结底，你要让es性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半

比如说，你一共要在es中存储1T的数据，那么你的多台机器留个filesystem cache的内存加起来综合，至少要到512G，至少半数的情况下，搜索是走内存的，性能一般可以到几秒钟，2秒，3秒，5秒

如果最佳的情况下，我们自己的生产环境实践经验，所以说我们当时的策略，是仅仅在es中就存少量的数据，就是你要用来搜索的那些索引，内存留给filesystem cache的，就100G，那么你就控制在100gb以内，相当于是，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在1秒以内

> 举例说明

比如说你现在有一行数据

id name age ....30个字段

但是你现在搜索，只需要根据id name age三个字段来搜索

如果你傻乎乎的往es里写入一行数据所有的字段，就会导致说70%的数据是不用来搜索的，结果硬是占据了es机器上的filesystem cache的空间，单挑数据的数据量越大，就会导致filesystem cahce能缓存的数据就越少

仅仅只是写入es中要用来检索的少数几个字段就可以了，比如说，就写入es id name age三个字段就可以了，然后你可以把其他的字段数据存在mysql里面，我们一般是**建议用es + hbase的这么一个架构**。

- hbase的特点是适用于海量数据的在线存储，就是对hbase可以写入海量数据，不要做复杂的搜索，就是做很简单的一些根据id或者范围进行查询的这么一个操作就可以了

从es中根据name和age去搜索，拿到的结果可能就20个doc id，然后根据doc id到hbase里去查询每个doc id对应的完整的数据，给查出来，再返回给前端。

你最好是写入es的数据小于等于，或者是略微大于es的filesystem cache的内存容量

然后你从es检索可能就花费20ms，然后再根据es返回的id去hbase里查询，查20条数据，可能也就耗费个30ms，可能你原来那么玩儿，1T数据都放es，会每次查询都是5~10秒，现在可能性能就会很高，每次查询就是50ms。

elastcisearch减少数据量仅仅放要用于搜索的几个关键字段即可，尽量写入es的数据量跟es机器的filesystem cache是差不多的就可以了；其他不用来检索的数据放hbase里，或者mysql。

**（2）数据预热**

假如说，哪怕是你就按照上述的方案去做了，es集群中每个机器写入的数据量还是超过了filesystem cache一倍，比如说你写入一台机器60g数据，结果filesystem cache就30g，还是有30g数据留在了磁盘上。

举个例子，就比如说，微博，你可以把一些大v，平时看的人很多的数据给提前你自己后台搞个系统，每隔一会儿，你自己的后台系统去搜索一下热数据，刷到filesystem cache里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。

电商，你可以将平时查看最多的一些商品，比如说iphone 8，热数据提前后台搞个程序，每隔1分钟自己主动访问一次，刷到filesystem cache里去。

对于那些你觉得比较热的，经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据，每隔一段时间，你就提前访问一下，让数据进入filesystem cache里面去。这样期待下次别人访问的时候，一定性能会好一些。

**（3）冷热分离**

关于es性能优化，数据拆分，我之前说将大量不搜索的字段，拆分到别的存储中去，这个就是类似于后面我最后要讲的mysql分库分表的垂直拆分。

es可以做类似于mysql的水平拆分，就是说将大量的访问很少，频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引

你最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在filesystem os cache里，别让冷数据给冲刷掉。

你看，假设你有6台机器，2个索引，一个放冷数据，一个放热数据，每个索引3个shard

3台机器放热数据index；另外3台机器放冷数据index

然后这样的话，你大量的时候是在访问热数据index，热数据可能就占总数据量的10%，此时数据量很少，几乎全都保留在filesystem cache里面了，就可以确保热数据的访问性能是很高的。

但是对于冷数据而言，是在别的index里的，跟热数据index都不再相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就10%的人去访问冷数据；90%的人在访问热数据。

**（4）document模型设计** 

案例分析：mysql有两张表

```mysql
订单表：id order_code total_price
        1  测试订单    5000
订单条目表：id order_id goods_id purchase_count price
            1  1         1        2            2000
            2  1         2        5            200
            
我在mysql里，都是
select * from order join order_item on order.id=order_item.order_id where order.id=1

1 测试订单 5000 1 1 1 2 2000
1 测试订单 5000 2 1 2 5 200
```

在es里该怎么玩儿，es里面的复杂的关联查询，复杂的查询语法，尽量别用，一旦用了性能一般都不太好

设计es里的数据模型

写入es的时候，搞成两个索引，order索引，orderItem索引

- order索引，里面就包含id order_code total_price
- orderItem索引，里面写入进去的时候，就完成join操作，id order_code total_price id order_id goods_id purchase_count price

写入es的java系统里，就完成关联，将关联好的数据直接写入es中，搜索的时候，就不需要利用es的搜索语法去完成join来搜索了

document模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es能支持的操作就是那么多，不要考虑用es做一些它不好操作的事情。如果真的有那种操作，尽量在document模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如join，nested，parent-child搜索都要尽量避免，性能都很差的。

**很多复杂的乱七八糟的一些操作，如何执行**

两个思路，在搜索/查询的时候，要执行一些业务强相关的特别复杂的操作：

1）在写入数据的时候，就设计好模型，加几个字段，把处理好的数据写入加的字段里面

2）自己用java程序封装，es能做的，用es来做，搜索出来的数据，在java程序里面去做，比如说我们，基于es，用java封装一些特别复杂的操作

**(5) 分页性能优化**

1. 不允许深度分页/默认深度分页性能很惨

   你系统不允许他翻那么深的页，告诉pm，默认翻的越深，性能就越差

2. 类似于app里的推荐商品不断下拉出来一页一页的

   类似于微博中，下拉刷微博，刷出来一页一页的，你可以用scroll api

   scroll会一次性给你生成所有数据的一个快照，然后每次翻页就是通过游标移动，获取下一页下一页这样子，性能会比上面说的那种分页性能也高很多很多

   针对这个问题，你可以考虑用scroll来进行处理，scroll的原理实际上是保留一个数据快照，然后在一定时间内，你如果不断的滑动往后翻页的时候，类似于你现在在浏览微博，不断往下刷新翻页。那么就用scroll不断通过游标获取下一页数据，这个性能是很高的，比es实际翻页要好的多的多。

但是唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。同时这个scroll是要保留一段时间内的数据快照的，你需要确保用户不会持续不断翻页翻几个小时。

无论多少页，性能基本上都是毫秒级的

因为scroll api是只能一页一页往后翻的，是不能说，先进入第10页，然后去120页，回到58页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻

##### 2.6 ES集群部署架构是什么？索引数据量多少，分多少片

> 模拟版

1. es生产集群我们部署了5台机器，每台机器是6核64G的，集群总内存是320G
2. 我们es集群的日增量数据大概是2000万条，每天日增量数据大概是500MB，每月增量数据大概是6亿，15G。目前系统已经运行了几个月，现在es集群里数据总量大概是100G左右。
3. 目前线上有5个索引（这个结合你们自己业务来，看看自己有哪些数据可以放es的），每个索引的数据量大概是20G，所以这个数据量之内，我们每个索引分配的是8个shard，比默认的5个shard多了3个shard。

#### 3. 分布式缓存



### 性能监控

#### 1 常用监控手段

1. [阿里的云监控（CloudMonitor）](https://help.aliyun.com/product/28572.html?spm=a2c4g.11186623.6.540.2ef03ae4BZRAjm) 




### 相关文章

1. [learningnotes](http://moxi159753.gitee.io/learningnotes/#/) 